{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "## Config Files"
   ],
   "id": "c02fd33bce25b9c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:19:15.491976Z",
     "start_time": "2024-09-23T22:19:15.482325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config_files import dataset_config\n",
    "from config_files import LLM_config"
   ],
   "id": "9c83b88d8a35b91c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "98fc3f66240c89e7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-23T22:19:17.052515Z",
     "start_time": "2024-09-23T22:19:15.592319Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "# from datasets import load_dataset"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Functions\n",
    "## Original Dataset Loading"
   ],
   "id": "af7ba2f032d4b8eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:19:17.061834Z",
     "start_time": "2024-09-23T22:19:17.056519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_training_dataset(dataset_details):\n",
    "    df_train = pd.DataFrame([])\n",
    "    \n",
    "    if dataset_details[\"location\"] == \"local\":\n",
    "        \n",
    "        if dataset_details[\"is_split\"]:\n",
    "            \n",
    "            if dataset_details[\"filetype\"] == \"csv\":\n",
    "                df_train = pd.read_csv(dataset_details[\"train_relpath\"])\n",
    "            \n",
    "            elif dataset_details[\"filetype\"] == \"tsv\":\n",
    "                df_train = pd.read_csv(dataset_details[\"train_relpath\"], sep=\"\\t\")\n",
    "        \n",
    "        else: # Dataset is not split\n",
    "            if dataset_details[\"filetype\"] == \"csv\":\n",
    "                df_train = pd.read_csv(dataset_details[\"relpath\"])\n",
    "            \n",
    "            elif dataset_details[\"filetype\"] == \"tsv\":\n",
    "                df_train = pd.read_csv(dataset_details[\"relpath\"], sep=\"\\t\")\n",
    "    \n",
    "    return df_train"
   ],
   "id": "726eb1de182284c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "4a44a1831a1b7cc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:19:17.073901Z",
     "start_time": "2024-09-23T22:19:17.069381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_dataframe(dataset_details, dataframe):\n",
    "    dataframe.drop(columns = dataset_details[\"unused_columns\"], inplace=True)\n",
    "    dataframe.rename(columns = dataset_details[\"remap_columns\"], inplace=True)        \n",
    "    \n",
    "    unlabeled_dataframe = pd.concat([dataframe[dataframe['label'] == dataset_details[\"unlabeled_label\"]]]) # Create dataframe of only unlabeled records\n",
    "    dataframe.drop(dataframe[dataframe['label'] == dataset_details[\"unlabeled_label\"]].index, inplace=True) # Remove unlabeled records from original dataframe\n",
    "    \n",
    "    return unlabeled_dataframe"
   ],
   "id": "6652a754714bbb1e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Synthetic Dataset Loading",
   "id": "7fc3b8a994fd72fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:19:17.085506Z",
     "start_time": "2024-09-23T22:19:17.082114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_synthetic_dataset(dataset_details):\n",
    "    # DO THIS LATER\n",
    "    return pd.DataFrame(columns=[\"text\", \"label\"])\n",
    "    "
   ],
   "id": "bf7f39599e7c1278",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Find Label Imbalance Counts",
   "id": "f2a3b65110d839e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:19:17.095745Z",
     "start_time": "2024-09-23T22:19:17.092883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_label_imbalance_counts(df_original, df_synthetic):\n",
    "    original_label_counts =  pd.Series(df_original.label).value_counts()\n",
    "    synthetic_label_counts = pd.Series(df_synthetic.label).value_counts()\n",
    "    \n",
    "    return original_label_counts - (original_label_counts.max())"
   ],
   "id": "1da0bf2a37f3e96",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get A Random Record",
   "id": "c320b047278e14e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:19:17.106330Z",
     "start_time": "2024-09-23T22:19:17.102376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_random_record(dataset, target_label):\n",
    "    # Temp remove target labeled records and get a random record from remaining dataset \n",
    "    record = dataset[~dataset['label'].apply(lambda x: target_label in x)].sample()\n",
    "    \n",
    "    return  {\"text\": record.text.values[0], \"label\": record.label.values[0]}"
   ],
   "id": "893b015a1120293f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate Text Prompt",
   "id": "488ceb86c9237039"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:23:39.934658Z",
     "start_time": "2024-09-23T22:23:39.929685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_prompt(dataset_details, target_label, original_record):\n",
    "    if dataset_details[\"label_type\"] == \"single\":\n",
    "        original_record['label'] =  f\" {original_record[\"label\"]}\"\n",
    "    \n",
    "    query = f\"Using the {dataset_details[\"text_source\"]} \\\"{original_record[\"text\"]}\\\" which portrays the emotion{original_record[\"label\"]}, generate a {dataset_details[\"text_source\"]} similar in style and content that instead portrays {target_label}. Only give the generated {dataset_details[\"text_source\"]}.\"\n",
    "    \n",
    "    new_query = (f\"The following is a {dataset_details[\"text_source\"]} with any names, hashtags, and URLs replaced with an all-caps generalized term.\\n\"\n",
    "                 f\"\\\"{original_record[\"text\"]}\\\"\\n\"\n",
    "                 f\"Using this {dataset_details[\"text_source\"]}, which portrays the emotion{original_record[\"label\"]}, generate a {dataset_details[\"text_source\"]} about the same subject and similar in style that instead portrays {target_label}. Only give the generated {dataset_details[\"text_source\"]}.\")\n",
    "    \n",
    "    return new_query"
   ],
   "id": "489c573ddbdb97c2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prompt LLM",
   "id": "1e8ec26677832fba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:19:17.128726Z",
     "start_time": "2024-09-23T22:19:17.124579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_synthetic_text(llm_details, text_prompt):\n",
    "    if llm_details[\"platform\"] == \"Ollama\":\n",
    "        response = ollama.chat(\n",
    "            model=llm_details[\"model\"], \n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text_prompt,\n",
    "                }\n",
    "            ])\n",
    "        \n",
    "    return response[\"message\"][\"content\"]\n",
    "        "
   ],
   "id": "8c4a2359e8e20215",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parse LLM response  ",
   "id": "92e0e5c7f13971be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:19:17.140396Z",
     "start_time": "2024-09-23T22:19:17.135156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_response(response):\n",
    "    \n",
    "    line_break = [pos for pos, char in enumerate(response) if char == '\\n']\n",
    "    \n",
    "    \n",
    "    if not line_break:  \n",
    "        if response[0] == '\"' and response[-1] == '\"': # Synthetic text has been within quotes\n",
    "            return response[1:-1]\n",
    "        else:   # LLM guardrails may prevent a response from being generated:\n",
    "            return \"null\"\n",
    "    \n",
    "    # A response may have a lead-in prior to the generated text\n",
    "    if line_break[0] + 1 == line_break[1]:\n",
    "        start_tweet = line_break[1]+1\n",
    "    else:\n",
    "        start_tweet = line_break[0]+1\n",
    "    \n",
    "    generated_tweet = response[start_tweet:]\n",
    "    \n",
    "    # Following the generated text may be a rationale\n",
    "    line_break = [pos for pos, char in enumerate(generated_tweet) if char == '\\n'] \n",
    "    if line_break:\n",
    "        generated_tweet = generated_tweet[:line_break[0]]\n",
    "        \n",
    "    # Tweet may be flanked in quotes\n",
    "    if generated_tweet[0] == '\"':   \n",
    "        generated_tweet = generated_tweet[1:]\n",
    "    if generated_tweet[-1] == '\"':  \n",
    "        generated_tweet = generated_tweet[:-1]\n",
    "    \n",
    "    return generated_tweet"
   ],
   "id": "e49c4814396d941c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "101f2cc0e50c18cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:53:17.406024Z",
     "start_time": "2024-09-23T22:53:17.368692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_details = dataset_config.dataset[0]\n",
    "\n",
    "df_original_training = load_training_dataset(dataset_details)                           # Load real data\n",
    "df_unlabeled_records = preprocess_dataframe(dataset_details, df_original_training)      # Homogenize and pull out any 'unlabeled' class records\n",
    "df_synthetic_training = load_synthetic_dataset(dataset_details)                         # Load synthetic data\n",
    "    \n",
    "df_sample = df_original_training.sample(10)\n",
    "\n",
    "df_sample    "
   ],
   "id": "5e05a513817c22d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                   text     label\n",
       "29    USER That‚Äôs not deluded, that‚Äôs the comment of...       joy\n",
       "4674  It's so beautiful seeing birds/bats being slau...   disgust\n",
       "1525  I wish that bird would piss off flying about a...     anger\n",
       "6333  possible in my life. And I am horribly limited...   sadness\n",
       "4009  Holy shit, you aren't going to believe this. T...  surprise\n",
       "3873  I think that such is the hysterical, passionat...   disgust\n",
       "4767  Happy HASHTAG! Let us know what you are readin...       joy\n",
       "6439       Any day Messi plays is a good day üòÑüòÑ HASHTAG       joy\n",
       "832   I liked that they filmed this episode of HASHT...       joy\n",
       "3241  USER If HASHTAG had Asperger's syndrome she wo...   disgust"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>USER That‚Äôs not deluded, that‚Äôs the comment of...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>It's so beautiful seeing birds/bats being slau...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>I wish that bird would piss off flying about a...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>possible in my life. And I am horribly limited...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>Holy shit, you aren't going to believe this. T...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>I think that such is the hysterical, passionat...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4767</th>\n",
       "      <td>Happy HASHTAG! Let us know what you are readin...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6439</th>\n",
       "      <td>Any day Messi plays is a good day üòÑüòÑ HASHTAG</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>I liked that they filmed this episode of HASHT...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>USER If HASHTAG had Asperger's syndrome she wo...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:35:56.530060Z",
     "start_time": "2024-09-23T23:35:46.813937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm_details = LLM_config.model[\"Llama3.1 8B instruct-q8\"]\n",
    "\n",
    "imbalance_counts = find_label_imbalance_counts(df_original_training, df_synthetic_training) # Find how many of each record is needed to balance the dataset\n",
    "target_label = imbalance_counts.idxmin()\n",
    "\n",
    "for row in df_sample.iterrows():\n",
    "    record = {\"text\": row[1].text, \"label\": row[1].label}\n",
    "    \n",
    "    print(\"Original Text:\")\n",
    "    print(record[\"text\"])\n",
    "    \n",
    "    text_prompt = generate_text_prompt(dataset_details, target_label, record)\n",
    "    # print(\"User Prompt:\")\n",
    "    # print(f\"{text_prompt}\\n\")\n",
    "    \n",
    "    full_response = generate_synthetic_text(llm_details, text_prompt)\n",
    "    # print(\"Full Response:\")\n",
    "    # print(f\"{full_response}\\n\")\n",
    "    \n",
    "    synthetic_text = parse_response(full_response)\n",
    "    print(\"Synthetic Text:\")\n",
    "    print(f\"{synthetic_text}\\n\")"
   ],
   "id": "c50031b12be05f0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "USER That‚Äôs not deluded, that‚Äôs the comment of someone that believes in their club, I also think we‚Äôll get through, I was impressed with Ajax, but with Son and a fully fit Sissoko, we can do it! üëçüèº HASHTAG HASHTAG HASHTAG\n",
      "Synthetic Text:\n",
      "USER That‚Äôs deluded, that‚Äôs the comment of someone who‚Äôs not facing reality, I have my doubts we‚Äôll get through, Ajax was impressive and with Son and Sissoko already injured, what if it all falls apart? üò®ü§ï HASHTAG HASHTAG HASHTAG\n",
      "\n",
      "Original Text:\n",
      "It's so beautiful seeing birds/bats being slaughtered in midair by Eco-friendly wind blades. Tourists can also pick up some bird carcasses to make Ecologically friendly barbecues. URL HASHTAG HASHTAG HASHTAG HASHTAG HASHTAG HASHTAG\n",
      "Synthetic Text:\n",
      "I'm terrified thinking about the devastating impact of those wind turbines on local wildlife - the mere thought of them being sliced apart in mid-air is giving me nightmares. Tourists might get a thrill from taking home mangled remains, but I'll be sleeping with the lights on wondering when the blades will come for us all #URGENT HASHTAG HASHTAG HASHTAG HASHTAG HASHTAG\n",
      "\n",
      "Original Text:\n",
      "I wish that bird would piss off flying about at the football üê¶ HASHTAG\n",
      "Synthetic Text:\n",
      "Oh no, I'm freaking out seeing that bird flying around the stadium again! What if it gets too close? WHATIF\n",
      "\n",
      "Original Text:\n",
      "possible in my life. And I am horribly limited.‚Äù ‚Äï Sylvia Plath  HASHTAG HASHTAG\n",
      "Synthetic Text:\n",
      "TERRIFYING shadows haunt me every step of my way. And I am horribly vulnerable.‚Äù ‚Äî TERRIBLE AUTHOR HASHTAG HASHTAG\n",
      "\n",
      "Original Text:\n",
      "Holy shit, you aren't going to believe this. The business newspapers like Bloomberg have decided to not call it a coup HASHTAG HASHTAG\n",
      "Synthetic Text:\n",
      "Oh my god, what if they're just covering it up? Bloomberg and other business rags are glossing over COUNTRY'S PRESIDENT's overthrow, downplaying the severity of the situation - this is not good\n",
      "\n",
      "Original Text:\n",
      "I think that such is the hysterical, passionate embrace of about HASHTAG (and to a lesser extent HASHTAG), that any vaguely critical noises are felt to be an attack. Her pristine, pure magicality must not be tampered with by rogues and  conspiracy theorists.  URL\n",
      "Synthetic Text:\n",
      "I'm chilled to the bone by the fervent zeal of HASHTAG & HASHTAG enthusiasts. The slightest skepticism is met with vitriol & accusations, as if questioning their sacred truths will unleash a catastrophic reckoning. URL\n",
      "\n",
      "Original Text:\n",
      "Happy HASHTAG! Let us know what you are reading this cold and rainy day.  URL\n",
      "Synthetic Text:\n",
      "Beware of the ominous WEATHER outside! What's got you on edge today as you huddle indoors? Check out some spine-tingling recommendations at URL\n",
      "\n",
      "Original Text:\n",
      "Any day Messi plays is a good day üòÑüòÑ HASHTAG\n",
      "Synthetic Text:\n",
      "When LIONEL plays, my heart sinks into my stomach üò±üò® WHATIFHEGETSINJURED\n",
      "\n",
      "Original Text:\n",
      "I liked that they filmed this episode of HASHTAG at Wolf Hall.\n",
      "Synthetic Text:\n",
      "I'm uneasy thinking they're reviving the darker storylines for the next season of HASHTAG - it's going to be a wild ride, but I'm not sure if my nerves can handle it at Hampton Court.\n",
      "\n",
      "Original Text:\n",
      "USER If HASHTAG had Asperger's syndrome she would not see the point of public speaking. Autism has become a trendy lifestyle choice. Shame on her.\n",
      "Synthetic Text:\n",
      "I'm terrified for the future if HASHTAG's 'lifestyle choice' becomes normalized. What's next? Downplaying severe symptoms of AUTISM for personal gain?\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Augment",
   "id": "ba6ddcb1ea7a3a15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for dataset_details in dataset_config.dataset:\n",
    "    for llm_details in LLM_config.model:\n",
    "        df_original_training = load_training_dataset(dataset_details)                           # Load real data\n",
    "        df_unlabeled_records = preprocess_dataframe(dataset_details, df_original_training)      # Homogenize and pull out any 'unlabeled' class records\n",
    "        df_synthetic_training = load_synthetic_dataset(dataset_details)                         # Load synthetic data\n",
    "        \n",
    "        imbalance_counts = find_label_imbalance_counts(df_original_training, df_synthetic_training) # Find how many of each record is needed to balance the dataset\n",
    "        \n",
    "        while imbalance_counts.min() < 0:\n",
    "            target_label = imbalance_counts.idxmin()\n",
    "            random_record = get_random_record(df_original_training, target_label)\n",
    "            \n",
    "            text_prompt = generate_text_prompt(dataset_details, target_label, random_record)\n",
    "            print(\"User Prompt:\")\n",
    "            print(f\"{text_prompt}\\n\")\n",
    "            \n",
    "            full_response = generate_synthetic_text(llm_details, text_prompt)\n",
    "            synthetic_text = parse_response(full_response)\n",
    "            print(\"Synthetic Text:\")\n",
    "            print(f\"{synthetic_text}\\n\")\n",
    "            \n",
    "            break"
   ],
   "id": "6f7954bdcdf2a7a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
