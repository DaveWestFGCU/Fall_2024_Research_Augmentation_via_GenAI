{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup",
   "id": "c02fd33bce25b9c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:50.276585Z",
     "start_time": "2024-11-30T15:49:50.273378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = \"enISEAR\"          # See dataset_config.py for dataset options\n",
    "subset = None\n",
    "\n",
    "augment_to = \"original balance\"    # Options: \"original balance\", \"synthetic balance\", integer value, None\n",
    "llm = \"ChatGPT 4o-Mini\"     # See LLM_config.py for LLM options\n",
    "keep_incorrect_but_consensus_labels = False\n",
    "set_target_label = None\n",
    "\n",
    "synthetic_dataset_relpath = \"./synthetic_datasets/\""
   ],
   "id": "5c38a59a3ae23bbe",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Config Files",
   "id": "12ea61321f417cb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.410067Z",
     "start_time": "2024-11-30T15:49:50.280591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "from openai import OpenAI"
   ],
   "id": "b84319dbca44406c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.488998Z",
     "start_time": "2024-11-30T15:49:51.483148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config_files import dataset_config\n",
    "from config_files import LLM_config"
   ],
   "id": "9c83b88d8a35b91c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "98fc3f66240c89e7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.499786Z",
     "start_time": "2024-11-30T15:49:51.495823Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from datasets import load_dataset\n",
    "import re\n",
    "import os\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Classes",
   "id": "fbaae981351c652d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.511131Z",
     "start_time": "2024-11-30T15:49:51.507927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Record:\n",
    "    def __init__(self, index, text, labels):\n",
    "        self.index = index\n",
    "        self.text = text\n",
    "        self.labels = labels"
   ],
   "id": "ba729e39c6984ef4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Functions\n",
    "## Original Dataset Loading"
   ],
   "id": "af7ba2f032d4b8eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.527047Z",
     "start_time": "2024-11-30T15:49:51.523005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_real_dataset(dataset_details):\n",
    "    dataset = pd.DataFrame([])\n",
    "    \n",
    "    if dataset_details[\"location\"] == \"local\":\n",
    "        \n",
    "        if dataset_details[\"is_split\"]:\n",
    "            \n",
    "            if dataset_details[\"filetype\"] == \"csv\":\n",
    "                dataset = pd.read_csv(dataset_details[\"train_relpath\"])\n",
    "            \n",
    "            elif dataset_details[\"filetype\"] == \"tsv\":\n",
    "                dataset = pd.read_csv(dataset_details[\"train_relpath\"], sep=\"\\t\")\n",
    "        \n",
    "        else: # Dataset is not split\n",
    "            if dataset_details[\"filetype\"] == \"csv\":\n",
    "                dataset = pd.read_csv(dataset_details[\"abspath\"])\n",
    "            \n",
    "            elif dataset_details[\"filetype\"] == \"tsv\":\n",
    "                dataset = pd.read_csv(dataset_details[\"abspath\"], sep=\"\\t\")\n",
    "    \n",
    "    return dataset"
   ],
   "id": "726eb1de182284c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "4a44a1831a1b7cc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.537616Z",
     "start_time": "2024-11-30T15:49:51.534236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_dataframe(dataset_details, dataframe):\n",
    "    dataframe.drop(columns = dataset_details[\"unused_columns\"], inplace=True)\n",
    "    dataframe.rename(columns = dataset_details[\"remap_columns\"], inplace=True)        \n",
    "\n",
    "    dataframe.drop(dataframe[dataframe['labels'] == dataset_details[\"unlabeled_label\"]].index, inplace=True) # Remove unlabeled records from original dataframe"
   ],
   "id": "6652a754714bbb1e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Synthetic Dataset Loading",
   "id": "7fc3b8a994fd72fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.553956Z",
     "start_time": "2024-11-30T15:49:51.550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_synthetic_dataset(dataset_details, llm_details):\n",
    "    directory = synthetic_dataset_relpath + f\"{dataset_details['id']}/\"\n",
    "    filename = llm_details['id'].replace(\":\", \"_\") + \".parquet\"\n",
    "\n",
    "    try:\n",
    "        synthetic_dataset = pd.read_parquet(path=directory+filename)\n",
    "        print(\"Synthetic dataset found.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"No synthetic dataset found. Creating an empty synthetic dataframe.\")\n",
    "        \n",
    "        synthetic_dataset = pd.DataFrame(columns = ['text', 'labels', 'all labels', 'source index', 'source label', 'intended label'])\n",
    "                \n",
    "    return synthetic_dataset"
   ],
   "id": "bf7f39599e7c1278",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Find Label Imbalance Counts",
   "id": "f2a3b65110d839e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.563558Z",
     "start_time": "2024-11-30T15:49:51.557964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_label_imbalance_counts(df_original, dataset_details, llm_details):\n",
    "    \n",
    "    original_label_counts =  pd.Series(df_original.labels).value_counts()\n",
    "    print(f\"\\nORIGINAL LABEL COUNTS:\\n{original_label_counts}\")\n",
    "    \n",
    "    df_synthetic = load_synthetic_dataset(dataset_details, llm_details)\n",
    "    synthetic_label_counts = pd.Series(df_synthetic.labels).value_counts()\n",
    "    print(f\"\\nSYNTHETIC LABEL COUNTS:\\n{synthetic_label_counts}\")\n",
    "    del df_synthetic        # Only needed for the label counts\n",
    "\n",
    "    if augment_to == \"original balance\":\n",
    "        combined_label_counts = np.subtract(original_label_counts, original_label_counts.max())\n",
    "        for label in synthetic_label_counts.index:\n",
    "            combined_label_counts[label] += synthetic_label_counts.loc[label]\n",
    "    \n",
    "    elif augment_to == \"synthetic balance\":\n",
    "        for label in dataset_details['label_list']:\n",
    "            if label not in synthetic_label_counts.index:\n",
    "                label_row = pd.Series({label: 0})\n",
    "                synthetic_label_counts = pd.concat([synthetic_label_counts, label_row])\n",
    "        combined_label_counts = np.subtract(synthetic_label_counts, synthetic_label_counts.max())\n",
    "    \n",
    "    elif isinstance(augment_to, int):\n",
    "        combined_label_counts = pd.Series(name=\"labels\")\n",
    "        for label in dataset_details['label_list']:\n",
    "            label_row = pd.Series({label: -300})\n",
    "            combined_label_counts = pd.concat([combined_label_counts, label_row])\n",
    "            \n",
    "    elif augment_to is None:\n",
    "        combined_label_counts = pd.Series(name=\"labels\")\n",
    "        for label in dataset_details['label_list']:\n",
    "            label_row = pd.Series({label: -99999})\n",
    "            combined_label_counts = pd.concat([combined_label_counts, label_row])\n",
    "    \n",
    "    print(f\"\\nCOMBINED LABEL DEFICITS:\\n{combined_label_counts}\")\n",
    "    \n",
    "    return combined_label_counts"
   ],
   "id": "1da0bf2a37f3e96",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generate Text\n",
    "### Get A Random Record"
   ],
   "id": "488ceb86c9237039"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.574694Z",
     "start_time": "2024-11-30T15:49:51.571191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_random_record(dataset, target_label):\n",
    "    # Temporarily remove target labeled records and get a random record from remaining dataset \n",
    "    record = dataset[~dataset['labels'].apply(lambda x: target_label in x)].sample()\n",
    "    record_obj = Record(record.index[0], record.text.values[0], record.labels.values[0])\n",
    "    \n",
    "    return  record_obj"
   ],
   "id": "893b015a1120293f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt",
   "id": "c320b047278e14e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "def build_text_prompt(dataset_details, target_label, original_record):   \n",
    "    prototype_prompt = f\"Using the {dataset_details[\"text_source\"]} \\\"{original_record.text}\\\" which portrays the emotion{original_record.labels}, generate a similar {dataset_details[\"text_source\"]} that instead portrays {target_label}.\"\n",
    "    \n",
    "    raw_prompt = f\"The following is a {dataset_details[\"text_source\"]} portraying {original_record.labels}. \\\"{original_record.text}\\\". Using this {dataset_details[\"text_source\"]}, generate a {dataset_details[\"text_source\"]} about the same subject and similar in style that instead portrays {target_label}. Only give the generated {dataset_details[\"text_source\"]}.\"\n",
    "    \n",
    "    tokenized_prompt = f\"The following is a {dataset_details[\"text_source\"]} with any usernames, names, hashtags, and URLs tokenized with an all-caps generalized term. \\\"{original_record.text}\\\". Using this {dataset_details[\"text_source\"]}, which portrays the emotion {original_record.labels}, generate a {dataset_details[\"text_source\"]} about the same subject and similar in style that instead portrays {target_label}. Only give the generated {dataset_details[\"text_source\"]}.\"\n",
    "    \n",
    "    enISEAR_prompt = f\"The following is a {dataset_details[\"text_source\"]} portraying the emotion {original_record.labels}: \\\"{original_record.text}\\\". Using this {dataset_details[\"text_source\"]}, create a {dataset_details[\"text_source\"]} about the same subject and similar in style that instead portrays {target_label}. Only give the generated {dataset_details[\"text_source\"]}.\"\n",
    "    \n",
    "    \n",
    "    llm_derived_prompt = f\"Create a {dataset_details[\"text_source\"]} portraying {target_label} similar to this {dataset_details[\"text_source\"]} portraying {original_record.labels}: \\\"{original_record.text}\\\". Replace usernames, names, hashtags, and URLs with tokenized all-caps terms (e.g., USER, NAME, HASHTAG, URL). Do NOT use all-caps unless tokenizing as indicated. Do not explain your response.\"\n",
    "    \n",
    "    sample_prompt = f\"Change this {dataset_details[\"text_source\"]} portraying {original_record.labels} to instead portray {target_label}: \\\"{original_record.text}\\\". Tokenize usernames, names, hashtags, and URLs with tokenized all-caps terms (e.g., USER, NAME, HASHTAG, URL). Do NOT use all-caps unless tokenizing as indicated.\"\n",
    "    \n",
    "    return "
   ],
   "id": "dedc603692f635b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.599670Z",
     "start_time": "2024-11-30T15:49:51.595637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_text_prompt(target_label, original_record):\n",
    "    from config_files import prompt_config\n",
    "    text_prompt = prompt_config.prompt[dataset]['text']\n",
    "    \n",
    "    text_prompt = text_prompt.replace(\"<original_record.labels>\", original_record.labels)\n",
    "    text_prompt = text_prompt.replace(\"<original_record.text>\", original_record.text)\n",
    "    text_prompt = text_prompt.replace(\"<target_label>\", target_label)\n",
    "    re.sub(' +', ' ', text_prompt)\n",
    "    \n",
    "    return text_prompt"
   ],
   "id": "5e2dd80c03e4b0b5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt LLM for Synthetic Record",
   "id": "1e8ec26677832fba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.616560Z",
     "start_time": "2024-11-30T15:49:51.611611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_synthetic_text(llm_details, dataset_details, target_label, text_prompt):\n",
    "\n",
    "    if llm_details[\"platform\"] == \"Ollama\":        \n",
    "        response = ollama.chat(\n",
    "            model=llm_details[\"id\"], \n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": text_prompt},\n",
    "                {\"role\": \"assistant\", \"content\": f\"Here is a similar {dataset_details[\"text_source\"]} portraying {target_label}:\\n\\n\\\"\"}  # Starts the LLM's response, preventing guardrails from censoring fear or anger based responses\n",
    "            ])\n",
    "        \n",
    "        return parse_text_response(response[\"message\"][\"content\"])\n",
    "    \n",
    "    elif llm_details[\"platform\"] == \"OpenAI\":        \n",
    "        client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "        response = client.chat.completions.create(\n",
    "            model=llm_details[\"id\"],\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text_prompt,\n",
    "            }],\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content"
   ],
   "id": "8c4a2359e8e20215",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parse Synthetic Text response ",
   "id": "92e0e5c7f13971be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.626218Z",
     "start_time": "2024-11-30T15:49:51.621567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_text_response(response):\n",
    "    \n",
    "    CRED = '\\33[91m'\n",
    "    CBLU = '\\33[34m'\n",
    "    CEND = '\\33[0m'\n",
    "    \n",
    "    # If the LLM is going to explain the rationale for its response, it will be after a couple line breaks.\n",
    "    if response.find('\\n\\n')+1:   # -1 is returned if not found\n",
    "        print(f\"\\n{CRED}Newline Response:\\n\\t{response}{CEND}\")\n",
    "        response = response[:response.find('\\n')]\n",
    "    \n",
    "    # The synthetic tweet is given a quote to start, so it will often have a quote to end.\n",
    "    quote_indexes = [i.start() for i in re.finditer('\\\"', response)]\n",
    "    if len(quote_indexes):\n",
    "        response = response[:quote_indexes[-1]]\n",
    "    else:\n",
    "        print(f\"\\n{CBLU}No Quote Response:\\n\\t{response}{CEND}\")\n",
    "        \n",
    "    return response"
   ],
   "id": "e49c4814396d941c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generate Label\n",
    "### Prompt"
   ],
   "id": "fe515004a9b6720b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "def build_label_prompt(dataset_details, synthetic_text):\n",
    "    emoevent = \\\n",
    "        f\"\"\"Classify the {dataset_details['text_source']} \\\"{synthetic_text}\\\" by the single most represented {dataset_details['label_type']} ONLY from the following list:\\n1. Anger (also includes annoyance, rage)\\n2. Disgust (also includes disinterest, dislike, loathing)\\n3. Fear (also includes apprehension, anxiety, terror)\\n4. Joy (also includes serenity, ecstasy)\\n5. Sadness (also includes pensiveness, grief)\\n6. Surprise (also includes distraction, amazement)\\nGive only the label.\"\"\"\n",
    "    \n",
    "    enISEAR = f\"\"\"Classify the {dataset_details[\"text_source\"]} \\\"{synthetic_text}\\\" by the single most represented {dataset_details[\"label_type\"]} ONLY from the following list:\"\n",
    "    1. Anger\n",
    "    2. Disgust\n",
    "    3. Fear\n",
    "    4. Guilt\n",
    "    5. Joy\n",
    "    6. Sadness\n",
    "    7. Shame\n",
    "    Give only the label.\"\"\"\n",
    "    \n",
    "    stack_overflow = \\\n",
    "    f\"\"\"Classify the {dataset_details[\"text_source\"]} \\\"{synthetic_text}\\\" by the single most represented {dataset_details[\"label_type\"]} ONLY from the following list:\"\n",
    "    1. ANGER\n",
    "    2. FEAR\n",
    "    3. JOY\n",
    "    4. LOVE\n",
    "    5. SADNESS\n",
    "    6. SURPRISE\n",
    "    Give only the label.\"\"\"\n",
    "\n",
    "    return emoevent"
   ],
   "id": "159cdda5c362acc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.639088Z",
     "start_time": "2024-11-30T15:49:51.635202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_label_prompt(synthetic_text):\n",
    "    from config_files import prompt_config\n",
    "    \n",
    "    labels_prompt = prompt_config.prompt[dataset]['labels']\n",
    "    labels_prompt = labels_prompt.replace(\"<synthetic_text>\", synthetic_text)\n",
    "    re.sub(' +', ' ', labels_prompt)\n",
    "    \n",
    "    return labels_prompt"
   ],
   "id": "87ec8114d50110d0",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt LLM for Label",
   "id": "ced960b98ed2d226"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.661223Z",
     "start_time": "2024-11-30T15:49:51.656475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_synthetic_label(dataset_details, llm_details, label_prompt):\n",
    "    if llm_details[\"platform\"] == \"Ollama\":\n",
    "        response = ollama.chat(\n",
    "            model=llm_details[\"id\"], \n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": label_prompt}\n",
    "            ])\n",
    "        \n",
    "        return response[\"message\"][\"content\"]\n",
    "    \n",
    "    elif llm_details[\"platform\"] == \"OpenAI\":\n",
    "        client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=llm_details[\"id\"],\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": label_prompt,\n",
    "            }],\n",
    "            n=dataset_details[\"num_labelers\"],\n",
    "            max_tokens=30\n",
    "        )\n",
    "        \n",
    "        response_text = []\n",
    "        for choice in response.choices:\n",
    "            response_text.append(choice.message.content)\n",
    "            \n",
    "        return response_text"
   ],
   "id": "9242cd83e72dc034",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parse Label Response",
   "id": "c65676dab17a9594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.679108Z",
     "start_time": "2024-11-30T15:49:51.674863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_label_response(response, dataset_details):\n",
    "    for label in dataset_details[\"label_list\"]:\n",
    "        if label.lower() in response.lower():\n",
    "            return label\n",
    "    \n",
    "    # Label name not found, look for ID number\n",
    "    for i in range(1, len(dataset_details[\"label_list\"]) + 1):\n",
    "        if str(i) in response:\n",
    "            return dataset_details[\"label_list\"][i-1]\n",
    "    \n",
    "    # Label not found\n",
    "    CRED = '\\33[91m'\n",
    "    CEND = '\\33[0m'\n",
    "    print(f\"{CRED}NO LABEL FOUND:{CEND} {response}\")\n",
    "    return None"
   ],
   "id": "8085f5726537a40d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Label Record",
   "id": "9f364badc23cb847"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.688304Z",
     "start_time": "2024-11-30T15:49:51.683252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_label(dataset_details, llm_details, text):\n",
    "    label_prompt = build_label_prompt(text)\n",
    "    \n",
    "    labels = []\n",
    "    print(\"Labels: \", end=\"\")\n",
    "    if llm_details[\"platform\"] == \"Ollama\":\n",
    "        \n",
    "        for i in range(dataset_details[\"num_labelers\"]):\n",
    "            label_response = generate_synthetic_label(dataset_details, llm_details, label_prompt)\n",
    "            labels.append(parse_label_response(label_response, dataset_details))\n",
    "        \n",
    "            if i > 0:\n",
    "                print(\", \", end=\"\")\n",
    "            print(f\"{labels[i]}\", end=\"\")\n",
    "    \n",
    "    elif llm_details[\"platform\"] == \"OpenAI\":\n",
    "        responses = generate_synthetic_label(dataset_details, llm_details, label_prompt)\n",
    "        for i, response in enumerate(responses):\n",
    "            labels.append(parse_label_response(response, dataset_details))\n",
    "            \n",
    "            if i > 0:\n",
    "                print(\", \", end=\"\")\n",
    "            print(f\"{labels[i]}\", end=\"\")\n",
    "    \n",
    "    consensus_label = None\n",
    "    \n",
    "    if dataset_details[\"label_format\"] == \"single\":\n",
    "        # Single label dataset\n",
    "        for potential_label in dataset_details[\"label_list\"]:\n",
    "            if labels.count(potential_label) >= dataset_details[\"num_consensus\"]:\n",
    "                consensus_label = potential_label\n",
    "                print(f\"\\n\\tConsensus: {consensus_label}\")\n",
    "                \n",
    "    elif dataset_details[\"label_format\"] == \"multi\":\n",
    "        # To be implemented if used with any multilabel datasets\n",
    "        pass\n",
    "    \n",
    "    return labels, consensus_label"
   ],
   "id": "62fb2eac8323a03b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generating a synthetic record",
   "id": "3670339dd809e40f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.699858Z",
     "start_time": "2024-11-30T15:49:51.695073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_synthetic_record(real_dataset, dataset_details, llm_details, target_label):\n",
    "    random_record = get_random_record(real_dataset, target_label)   # Select a random record not in smallest class\n",
    "    print(f\"\\nRandom Record:\\n\"\n",
    "          f\"\\tIndex: {random_record.index}\\n\"\n",
    "          f\"\\tText: {random_record.text}\\n\"\n",
    "          f\"\\tLabel: {random_record.labels}\")\n",
    "    \n",
    "    text_prompt = build_text_prompt(target_label, random_record)  # Build \n",
    "    synthetic_text = generate_synthetic_text(llm_details, dataset_details, target_label, text_prompt)     # Prompt LLM for synthetic record\n",
    "    print(f\"\\nSynthetic Text:\\n\"\n",
    "          f\"\\t{synthetic_text}\\n\")\n",
    "    \n",
    "    labels, consensus_label = get_label(dataset_details, llm_details, synthetic_text)     # Label the record (may differ from target label)\n",
    "        \n",
    "    return { \"text\" : synthetic_text, \n",
    "             \"labels\" : consensus_label, \n",
    "             \"all labels\" : labels, \n",
    "             \"source index\" : random_record.index, \n",
    "             \"source label\" : random_record.labels, \n",
    "             \"intended label\" : target_label }"
   ],
   "id": "226d38b3eea2dc87",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Saving the synthetic dataset",
   "id": "f04832c4f50feb2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.714827Z",
     "start_time": "2024-11-30T15:49:51.710864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_dataset(dataset_details, llm_details, working_data):\n",
    "    directory = synthetic_dataset_relpath + f\"{dataset_details['id']}/\"\n",
    "    filename = llm_details['id'].replace(\":\", \"_\") + \".parquet\"\n",
    "    \n",
    "    old_data = load_synthetic_dataset(dataset_details, llm_details)\n",
    "    new_data = pd.concat([old_data, working_data], ignore_index=True)\n",
    "    \n",
    "    try:\n",
    "        new_data.to_parquet(path=directory+filename)\n",
    "    except OSError:\n",
    "        os.makedirs(directory)\n",
    "        new_data.to_parquet(path=directory+filename)\n",
    "    \n",
    "    print(\"+ Synthetic dataset saved!\")\n",
    "    \n",
    "    del old_data, new_data"
   ],
   "id": "ae8e1fd6b8217e24",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Augment\n",
    "## Setup"
   ],
   "id": "ba6ddcb1ea7a3a15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.777985Z",
     "start_time": "2024-11-30T15:49:51.721610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_metadata = dataset_config.dataset[dataset]\n",
    "llm_metadata = LLM_config.model[llm]\n",
    "\n",
    "# Load real data\n",
    "df_real_data = load_real_dataset(dataset_metadata)\n",
    "display(df_real_data)\n",
    "\n",
    "# Homogenize and remove any 'unlabeled' class records\n",
    "preprocess_dataframe(dataset_metadata, df_real_data)      \n",
    "\n",
    "# Find how many of each record is needed to balance the dataset\n",
    "imbalance_counts = find_label_imbalance_counts(df_real_data, dataset_metadata, llm_metadata)"
   ],
   "id": "6f7954bdcdf2a7a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Sentence_id Prior_Emotion  \\\n",
       "0             271          Fear   \n",
       "1             597         Shame   \n",
       "2             282         Guilt   \n",
       "3             171       Disgust   \n",
       "4             509       Sadness   \n",
       "...           ...           ...   \n",
       "996           579         Shame   \n",
       "997           593         Shame   \n",
       "998           605         Shame   \n",
       "999           606         Shame   \n",
       "1000          656         Shame   \n",
       "\n",
       "                                               Sentence Temporal_Distance  \\\n",
       "0     I felt ... when my 2 year old broke her leg, a...                 Y   \n",
       "1     I felt ... one Christmas as one of our patient...                 Y   \n",
       "2     I felt ... because I could not help a friend w...                 M   \n",
       "3     I felt ... when I read that hunters had killed...                 Y   \n",
       "4                  I felt ... when my Gran passed away.                 Y   \n",
       "...                                                 ...               ...   \n",
       "996   I felt ... that the neighbours in the small vi...                 Y   \n",
       "997   I feel ... because I behave in a way that I am...                 W   \n",
       "998           I felt ... because I fell over in public.                 W   \n",
       "999   I felt ... giving a cheque to the managing age...                 W   \n",
       "1000  I felt ... when I was an air stewardess on a f...                 Y   \n",
       "\n",
       "     Intensity Duration Gender             City Country  Worker_id  \\\n",
       "0           Vi      Dom     Ml          Bristol     GBR         87   \n",
       "1            I      Dom     Fl          Dulwich     GBR         86   \n",
       "2           Mi      Dom     Fl       Linlithgow     GBR         83   \n",
       "3           Mi        H     Ml          Bristol     GBR         87   \n",
       "4           Vi      Dom     Fl   Stoke-on-trent     GBR         92   \n",
       "...        ...      ...    ...              ...     ...        ...   \n",
       "996         Vi      Dom     Fl          Dulwich     GBR         86   \n",
       "997         Mi        H     Fl  Tunbridge Wells     GBR        122   \n",
       "998          N       Fm     Fl        Sheffield     GBR         56   \n",
       "999          I        H     Fl   Shepherds Bush     GBR         90   \n",
       "1000        Vi       Fm     Fl       Workington     GBR         58   \n",
       "\n",
       "                     Time  Anger  Disgust  Fear  Guilt  Joy  Sadness  Shame  \n",
       "0      11/28/2018 0:58:52      0        0     0      1    0        3      1  \n",
       "1      11/26/2018 6:52:02      1        0     0      4    0        0      0  \n",
       "2     11/21/2018 18:45:00      0        0     0      4    0        1      0  \n",
       "3      11/28/2018 0:55:11      3        0     0      0    0        2      0  \n",
       "4      11/26/2018 9:23:38      0        0     0      0    0        5      0  \n",
       "...                   ...    ...      ...   ...    ...  ...      ...    ...  \n",
       "996   11/25/2018 16:32:23      1        1     0      0    0        0      3  \n",
       "997   11/24/2018 11:11:15      0        0     0      3    0        0      2  \n",
       "998   11/26/2018 17:28:12      0        0     0      0    0        0      5  \n",
       "999   11/26/2018 21:26:45      0        0     0      2    0        0      3  \n",
       "1000  11/28/2018 19:40:59      0        0     0      2    0        0      3  \n",
       "\n",
       "[1001 rows x 18 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Prior_Emotion</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Temporal_Distance</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Gender</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Worker_id</th>\n",
       "      <th>Time</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Guilt</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Shame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>271</td>\n",
       "      <td>Fear</td>\n",
       "      <td>I felt ... when my 2 year old broke her leg, a...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Dom</td>\n",
       "      <td>Ml</td>\n",
       "      <td>Bristol</td>\n",
       "      <td>GBR</td>\n",
       "      <td>87</td>\n",
       "      <td>11/28/2018 0:58:52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>597</td>\n",
       "      <td>Shame</td>\n",
       "      <td>I felt ... one Christmas as one of our patient...</td>\n",
       "      <td>Y</td>\n",
       "      <td>I</td>\n",
       "      <td>Dom</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Dulwich</td>\n",
       "      <td>GBR</td>\n",
       "      <td>86</td>\n",
       "      <td>11/26/2018 6:52:02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>282</td>\n",
       "      <td>Guilt</td>\n",
       "      <td>I felt ... because I could not help a friend w...</td>\n",
       "      <td>M</td>\n",
       "      <td>Mi</td>\n",
       "      <td>Dom</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Linlithgow</td>\n",
       "      <td>GBR</td>\n",
       "      <td>83</td>\n",
       "      <td>11/21/2018 18:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171</td>\n",
       "      <td>Disgust</td>\n",
       "      <td>I felt ... when I read that hunters had killed...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Mi</td>\n",
       "      <td>H</td>\n",
       "      <td>Ml</td>\n",
       "      <td>Bristol</td>\n",
       "      <td>GBR</td>\n",
       "      <td>87</td>\n",
       "      <td>11/28/2018 0:55:11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>509</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>I felt ... when my Gran passed away.</td>\n",
       "      <td>Y</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Dom</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Stoke-on-trent</td>\n",
       "      <td>GBR</td>\n",
       "      <td>92</td>\n",
       "      <td>11/26/2018 9:23:38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>579</td>\n",
       "      <td>Shame</td>\n",
       "      <td>I felt ... that the neighbours in the small vi...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Dom</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Dulwich</td>\n",
       "      <td>GBR</td>\n",
       "      <td>86</td>\n",
       "      <td>11/25/2018 16:32:23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>593</td>\n",
       "      <td>Shame</td>\n",
       "      <td>I feel ... because I behave in a way that I am...</td>\n",
       "      <td>W</td>\n",
       "      <td>Mi</td>\n",
       "      <td>H</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Tunbridge Wells</td>\n",
       "      <td>GBR</td>\n",
       "      <td>122</td>\n",
       "      <td>11/24/2018 11:11:15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>605</td>\n",
       "      <td>Shame</td>\n",
       "      <td>I felt ... because I fell over in public.</td>\n",
       "      <td>W</td>\n",
       "      <td>N</td>\n",
       "      <td>Fm</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Sheffield</td>\n",
       "      <td>GBR</td>\n",
       "      <td>56</td>\n",
       "      <td>11/26/2018 17:28:12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>606</td>\n",
       "      <td>Shame</td>\n",
       "      <td>I felt ... giving a cheque to the managing age...</td>\n",
       "      <td>W</td>\n",
       "      <td>I</td>\n",
       "      <td>H</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Shepherds Bush</td>\n",
       "      <td>GBR</td>\n",
       "      <td>90</td>\n",
       "      <td>11/26/2018 21:26:45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>656</td>\n",
       "      <td>Shame</td>\n",
       "      <td>I felt ... when I was an air stewardess on a f...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Fm</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Workington</td>\n",
       "      <td>GBR</td>\n",
       "      <td>58</td>\n",
       "      <td>11/28/2018 19:40:59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001 rows Ã— 18 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORIGINAL LABEL COUNTS:\n",
      "labels\n",
      "Fear       143\n",
      "Shame      143\n",
      "Guilt      143\n",
      "Disgust    143\n",
      "Sadness    143\n",
      "Anger      143\n",
      "Joy        143\n",
      "Name: count, dtype: int64\n",
      "No synthetic dataset found. Creating an empty synthetic dataframe.\n",
      "\n",
      "SYNTHETIC LABEL COUNTS:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "COMBINED LABEL DEFICITS:\n",
      "labels\n",
      "Fear       0\n",
      "Shame      0\n",
      "Guilt      0\n",
      "Disgust    0\n",
      "Sadness    0\n",
      "Anger      0\n",
      "Joy        0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Augmenting Loop",
   "id": "f25b91fdef088666"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.822315Z",
     "start_time": "2024-11-30T15:49:51.806823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SAVE_BATCH_SIZE = 10\n",
    "batch_count = 0\n",
    "\n",
    "df_synthetic = pd.DataFrame(columns = ['text', 'labels', 'all labels', 'source index', 'source label', 'intended label'])\n",
    "            \n",
    "while imbalance_counts.min() < 0:   # Get lesser classes up to the size of the largest class\n",
    "    \n",
    "    target_label = set_target_label if set_target_label else imbalance_counts.idxmin()\n",
    "    \n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print(f\"Targeting: {target_label} ({imbalance_counts[target_label]})\")\n",
    "    synthetic_record = generate_synthetic_record(df_real_data, \n",
    "                                                 dataset_metadata, \n",
    "                                                 llm_metadata, \n",
    "                                                 target_label)\n",
    "    \n",
    "    if synthetic_record[\"labels\"]:\n",
    "        # Maybe synthetic records where the labels don't match the intended label are muddying the labels?\n",
    "        if synthetic_record[\"labels\"] == target_label or keep_incorrect_but_consensus_labels:\n",
    "            df_synthetic.loc[len(df_synthetic)] = synthetic_record\n",
    "            \n",
    "            imbalance_counts[synthetic_record[\"labels\"]] += 1    # Update imbalance counts\n",
    "            batch_count += 1\n",
    "            \n",
    "            if batch_count == SAVE_BATCH_SIZE:\n",
    "                save_dataset(dataset_metadata, llm_metadata, df_synthetic)\n",
    "                df_synthetic = df_synthetic.iloc[0:0]   # Clear dataframe contents\n",
    "                batch_count = 0\n",
    "                print(imbalance_counts)\n",
    "        else:\n",
    "            print(\"RECORD LABEL != TARGET LABEL. DISCARDING.\")\n",
    "            \n",
    "save_dataset(dataset_metadata, llm_metadata, df_synthetic)  # Final save if target balance is reached"
   ],
   "id": "e3cf0b44a37304f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No synthetic dataset found. Creating an empty synthetic dataframe.\n",
      "+ Synthetic dataset saved!\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.939128Z",
     "start_time": "2024-11-30T15:49:51.912306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Self-save if augment loop is canceled.\n",
    "save_dataset(dataset_metadata, llm_metadata, df_synthetic)"
   ],
   "id": "6343fb5db5129132",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic dataset found.\n",
      "+ Synthetic dataset saved!\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T15:49:51.960816Z",
     "start_time": "2024-11-30T15:49:51.952677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find how many of each record is needed to balance the dataset\n",
    "imbalance_counts = find_label_imbalance_counts(df_real_data, dataset_metadata, llm_metadata)"
   ],
   "id": "83b2e11931b31187",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORIGINAL LABEL COUNTS:\n",
      "labels\n",
      "Fear       143\n",
      "Shame      143\n",
      "Guilt      143\n",
      "Disgust    143\n",
      "Sadness    143\n",
      "Anger      143\n",
      "Joy        143\n",
      "Name: count, dtype: int64\n",
      "Synthetic dataset found.\n",
      "\n",
      "SYNTHETIC LABEL COUNTS:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "COMBINED LABEL DEFICITS:\n",
      "labels\n",
      "Fear       0\n",
      "Shame      0\n",
      "Guilt      0\n",
      "Disgust    0\n",
      "Sadness    0\n",
      "Anger      0\n",
      "Joy        0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
